{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comment fonctionne la transposition de texte ?\n",
    "\n",
    "1. Générer les fiches avec les nouvelles valeurs via build_report dans un dossier **template_dir**\n",
    "2. Télécharger toutes les fiches d'Osmose dans un dossier **modified_docx_dir** (créer ce dossier soi-même)\n",
    "3. Lancer ce notebook pour une transposition **modified_docx_dir** -> **transposed_docx_dir** en réutilisant les nouvelles fiches-templates générées dans **template_dir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import os\n",
    "import urllib.request\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "# Permet la génération de word\n",
    "import docx\n",
    "from docx import Document\n",
    "from docxcompose.composer import Composer\n",
    "from docxtpl import DocxTemplate, R, Listing, InlineImage\n",
    "from docx.shared import Mm\n",
    "\n",
    "# Parsing des commentaires\n",
    "from docx2python import docx2python\n",
    "\n",
    "# Nettoyage du texte\n",
    "import html\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dir = \"reports_word\"\n",
    "modified_docx_dir = \"modified_reports\"\n",
    "transposed_docx_dir = os.path.join(\"reports_word\", \"transposed_reports\")\n",
    "image_folder = os.path.join(\"reports_word\", \"reports_images\")\n",
    "avant_osmose = \"Fiche_Avant_Osmose\"\n",
    "\n",
    "def mkdir_ifnotexist(path) :\n",
    "    if not os.path.isdir(path) :\n",
    "        os.mkdir(path)\n",
    "        \n",
    "mkdir_ifnotexist(avant_osmose)    \n",
    "mkdir_ifnotexist(transposed_docx_dir)\n",
    "mkdir_ifnotexist(image_folder)\n",
    "mkdir_ifnotexist(modified_docx_dir)\n",
    "assert len(os.listdir(modified_docx_dir)) > 0, f\"Le dossier {modified_docx_dir} est vide. Vous devez y placer les fichiers docx contenant les commentaires à déplacer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_name(name):\n",
    "    # Normalise le nom de la mesure ou volet, notamment pour l'utiliser comme nom de code dans les commentaires\n",
    "    name = name.lower()\n",
    "    name = unidecode(name)\n",
    "    name = re.sub('[^a-z]', ' ',  name)\n",
    "    name = re.sub(' +', '', name)\n",
    "    return name\n",
    "\n",
    "\n",
    "# Mesures des fiches V2 contenant des commentaires\n",
    "volet2mesures = {\n",
    "    'Ecologie': [#'Bonus écologique',\n",
    "                  #\"MaPrimeRénov'\",\n",
    "                  #'Modernisation des filières automobiles et aéronautiques',\n",
    "                  #'Prime à la conversion des agroéquipements',\n",
    "                  #'Prime à la conversion des véhicules légers',\n",
    "                  #'Réhabilitation Friches (urbaines et sites pollués)',\n",
    "                  'Rénovation bâtiments Etat'],\n",
    " 'Compétitivité': ['AAP Industrie : Soutien aux projets industriels territoires',\n",
    "                  'AAP Industrie : Sécurisation approvisionnements critiques',\n",
    "                  'France Num : aide à la numérisation des TPE,PME,ETI',\n",
    "                  #'Industrie du futur',\n",
    "                  'Renforcement subventions Business France',\n",
    "                  #'Soutien aux filières culturelles (cinéma, audiovisuel, musique, numérique, livre)'\n",
    "                  ],\n",
    " 'Cohésion': [\n",
    "             #    'Apprentissage',\n",
    "             #     'Contrats Initiatives Emploi (CIE) Jeunes',\n",
    "             #     'Contrats de professionnalisation',\n",
    "             #     'Garantie jeunes',\n",
    "             #     'Parcours emploi compétences (PEC) Jeunes',\n",
    "             #     \"Prime à l'embauche des jeunes\",\n",
    "             #     \"Prime à l'embauche pour les travailleurs handicapés\",\n",
    "             #     'Service civique'\n",
    "             ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_prefixes = set(['Espace Commentaires\\xa0:', 'Espace Commentaires :', \n",
    "                        'Exemples de lauréats :', 'Exemples de lauréats\\xa0:', \n",
    "                       \"Commentaires généraux\\xa0:\", \"Commentaires généraux :\", \n",
    "                       \"Volet\\xa0: Ecologie\", \"Volet\\xa0: Compétitivité\", \"Volet\\xa0: Cohésion\",\n",
    "                       \"Volet : Ecologie\", \"Volet : Compétitivité\", \"Volet : Cohésion\",])\n",
    "\n",
    "def flatten(L):\n",
    "    # Aplatir une liste imbriquée [[., ., [[.]]]] -> [., ., .]\n",
    "    if type(L) is list:\n",
    "        for item in L:\n",
    "            yield from flatten(item)\n",
    "    else:\n",
    "        yield L\n",
    "        \n",
    "\n",
    "def gen_unit_list(L):\n",
    "    if (type(L) is list) and (len(L) > 0) and (type(L[0]) is not list):\n",
    "        yield L\n",
    "    else:\n",
    "        for item in L:\n",
    "            yield from gen_unit_list(item)\n",
    "\n",
    "\n",
    "def reformat_bullet_point(text):\n",
    "    text = re.sub('--\\t\\t', '- ', text)\n",
    "    text = re.sub('^--\\t-*', '- ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def reformat_url(text):\n",
    "    regex_clean = re.compile('<a href.*?>')\n",
    "    text = re.sub(regex_clean, '', text)\n",
    "    text = re.sub('</a>', \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def fix_vanishing_break_lines(text):\n",
    "    # Des retours à la ligne disparaissent lors de la lecture du docx\n",
    "    # via python2docx. Le constater en exécutant -> content = python2docx(filename); print(content.body)\n",
    "    text = re.sub('\\.  ', '.\\n\\n', text)\n",
    "    text = re.sub(': *-', ':\\n-', text)\n",
    "    text = re.sub(', *-', ',\\n-', text)\n",
    "    text = re.sub('; *-', ';\\n-', text)\n",
    "    text = re.sub('\\. *-', ';\\n-', text)\n",
    "    text = re.sub('€ *- ', '\\n- ', text)\n",
    "    text = re.sub('▪', '\\n▪', text)\n",
    "    return text\n",
    "\n",
    "    \n",
    "def extract_comment(textbox_content):    \n",
    "    # Cleaning section\n",
    "    texts = []\n",
    "    for text in textbox_content:\n",
    "        text = html.unescape(text)\n",
    "        text = reformat_bullet_point(text)\n",
    "        text = reformat_url(text)\n",
    "        text = fix_vanishing_break_lines(text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        texts.append(text)\n",
    "    textbox_content = texts\n",
    "\n",
    "    # Concatenation\n",
    "    textbox_content = [text.strip() for text in textbox_content]\n",
    "    textbox_content = '\\n'.join(textbox_content)\n",
    "    textbox_content = textbox_content.strip()\n",
    "\n",
    "    # Retirer un potentiel préfix (Espace Commentaires ...)\n",
    "    textbox_content = re.sub('^[0-9]+[\\r\\n]+[0-9]+', '', textbox_content).strip()\n",
    "    prefix_clean = False\n",
    "    while not prefix_clean:\n",
    "        # Les préfixes étant déclarés dans un set, dès lors qu'on retrouve un préfixe à retirer,\n",
    "        # on re-parcourt le set de préfixes \n",
    "        prefix_clean = True\n",
    "        for prefix in comment_prefixes:\n",
    "            if textbox_content.startswith(prefix):\n",
    "                textbox_content = textbox_content.replace(prefix, \"\", 1)\n",
    "                textbox_content = textbox_content.strip()\n",
    "                prefix_clean = False\n",
    "            if textbox_content.endswith(prefix):\n",
    "                textbox_content = textbox_content[:-len(prefix)]\n",
    "                textbox_content = textbox_content.strip()\n",
    "                prefix_clean = False\n",
    "            \n",
    "    \n",
    "    textbox_content = textbox_content.strip()\n",
    "    # Carriage pour conserver les retours à la ligne\n",
    "    textbox_content = re.sub(\"\\n\", \"\\r\\n\", textbox_content)\n",
    "    return textbox_content\n",
    "    \n",
    "\n",
    "def alternate_texts_and_images(doc, textbox_content):\n",
    "    r = re.compile(\"----media/(.*?)----\")  # Pattern pour les images\n",
    "    image_names = r.findall(textbox_content) + [None]\n",
    "    texts = r.split(textbox_content)\n",
    "    \n",
    "    frameworks = []\n",
    "    for text, image_basename in zip(texts[0::2], image_names):\n",
    "        if image_basename is not None:\n",
    "            image_path = os.path.join(image_folder, image_basename)\n",
    "            frameworks.append({'text': text, 'image': InlineImage(doc, image_path, height=Mm(40))})\n",
    "        else:\n",
    "            frameworks.append({'text': text, 'image': ''})\n",
    "    return frameworks\n",
    "\n",
    "\n",
    "def get_mesure_to_comment(doc, content, volet2mesures):\n",
    "    mesure2comment = {}\n",
    "    \n",
    "    # Pattern regex pour attraper le nom des mesures\n",
    "    list_mesures = [mesure for mesures in volet2mesures.values() for mesure in mesures]\n",
    "    re_group_mesures = \"(\"+ '|'.join(list_mesures) + \")\"\n",
    "    re_title_mesure_pattern = f'(\\d - <a href=.*>{re_group_mesures}</a>)'\n",
    "    \n",
    "    current_mesure = None\n",
    "    num_blocks_to_pass = 0\n",
    "    for text_list_block in content.body:\n",
    "        text_list = list(flatten(text_list_block))\n",
    "        if current_mesure is None:\n",
    "            # On veut le nom de la mesure\n",
    "            text_unit = \" \".join(text_list)\n",
    "            title_mesures = re.findall(re_title_mesure_pattern, text_unit)\n",
    "            if len(title_mesures) > 0:\n",
    "                current_mesure = title_mesures[0][1]\n",
    "                num_blocks_to_pass = 6\n",
    "        else:\n",
    "            # On veut récupérer le commentaire\n",
    "            # il faudra passer 3 tableaux + 3 retours à la ligne\n",
    "            if num_blocks_to_pass == 0:\n",
    "                \n",
    "                # On extrait le commentaire\n",
    "                text_list = list(flatten(text_list_block))\n",
    "                textbox_content = extract_comment(text_list)\n",
    "                frameworks = alternate_texts_and_images(doc, textbox_content)\n",
    "\n",
    "                # On associe volet et commentaire\n",
    "                encoded_mesure = encode_name(current_mesure)\n",
    "                mesure2comment[encoded_mesure] = frameworks\n",
    "                \n",
    "                current_mesure = None\n",
    "            \n",
    "            num_blocks_to_pass -= 1\n",
    "    \n",
    "    assert len(mesure2comment) == len(list_mesures), f\"{len(mesure2comment)} != {len(list_mesures)} attendues\"\n",
    "    return mesure2comment\n",
    "\n",
    "\n",
    "def get_volet_to_comment(doc, content, volet2mesures):\n",
    "    volet2comment = {}\n",
    "    body = content.body\n",
    "    volet = None\n",
    "    text_unit_generator = gen_unit_list(content.body)\n",
    "    volet_names_regex = \"(\" + '|'.join([volet for volet in volet2mesures]) + \")\"  # (Ecologie|Compétitivité|Cohésion)\n",
    "\n",
    "    for text_list in text_unit_generator:\n",
    "        # On cherche à trouver le titre contenant le nom du volet (partie else)\n",
    "        # puis on saura que le texte suivant contiendra le commentaire\n",
    "        if volet is not None:\n",
    "            # On extrait le commentaire\n",
    "            textbox_content = extract_comment(text_list)\n",
    "            frameworks = alternate_texts_and_images(doc, textbox_content)\n",
    "            \n",
    "            # On associe volet et commentaire\n",
    "            encoded_volet = encode_name(volet)\n",
    "            volet2comment[encoded_volet] = frameworks\n",
    "            \n",
    "            # Reinitialise volet\n",
    "            volet = None            \n",
    "        else:\n",
    "            text_list = ' '.join(text_list)\n",
    "            patterns = re.findall(f'(Volet [1-3] : {volet_names_regex})', text_list)\n",
    "            if len(patterns) > 0:\n",
    "                # On attrape le titre du volet et récupère le nom du volet\n",
    "                volet = patterns[-1][-1]\n",
    "    assert len(volet2comment) == 3\n",
    "    return volet2comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_comments(src_filename, template_filename, output_filename, volet2mesures, dict_cont):\n",
    "    # Lecture du document\n",
    "    content = docx2python(src_filename, image_folder=image_folder)\n",
    "    doc_template = DocxTemplate(template_filename)\n",
    "    \n",
    "    # Parse les commentaires sous les volets et mesures\n",
    "    mesure2comment = get_mesure_to_comment(doc_template, content, volet2mesures)\n",
    "    volet2comment = get_volet_to_comment(doc_template, content, volet2mesures)\n",
    "    context = {**mesure2comment, **volet2comment}\n",
    "    dep_name = output_filename.split('_')[-1].split('.docx')[0]\n",
    "    dict_cont[dep_name] = context\n",
    "\n",
    "    # On génère un nouveau document avec les commentaires recopiés\n",
    "    doc_template.render(context, autoescape=True)\n",
    "    doc_template.save(output_filename)\n",
    "    return output_filename, dict_cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template(template_filename, output_filename, volet2mesures, dict_cont):\n",
    "    ordered_mesures = [mesure for mesures in volet2mesures.values() for mesure in mesures]\n",
    "    ordered_volets = list(volet2mesures.keys())\n",
    "    \n",
    "    context = {encode_name(volet): [{'text': '', 'image': ''}] for volet in ordered_volets}\n",
    "    dep_name = output_filename.split('_')[-1].split('.docx')[0]\n",
    "    dict_cont[dep_name] = {}\n",
    "    doc = DocxTemplate(template_filename)\n",
    "    doc.render(context, autoescape=True)\n",
    "    doc.save(output_filename)\n",
    "    return output_filename, dict_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [os.path.join(template_dir, filename) for filename in os.listdir(template_dir) if filename.endswith('docx')]\n",
    "modified_docx = [os.path.join(modified_docx_dir, filename) for filename in os.listdir(modified_docx_dir) if filename.endswith('docx')]\n",
    "\n",
    "def map_templates_to_modified_reports(templates, modified_docx):\n",
    "    mapping = {filename:None for filename in templates}\n",
    "    \n",
    "    # Faire correspondre le nom des départements encodés vers le bon template\n",
    "    encoded_dep_name2template = {}\n",
    "    for filename in mapping:\n",
    "        raw_dep_name = filename.split('_')[-1].split('.')[0]\n",
    "        encoded_dep_name = encode_name(raw_dep_name)\n",
    "        encoded_dep_name2template[encoded_dep_name] = filename\n",
    "    assert len(encoded_dep_name2template) == 109, f'{len(encoded_dep_name2template)} départements trouvés'\n",
    "    \n",
    "    # Faire correspondre le nom du département\n",
    "    duplicated_dep = []\n",
    "    for modified in modified_docx:\n",
    "        content = docx2python(modified)\n",
    "        expr_with_dep_name = content.body[0][0][0][7]\n",
    "        print(f\"Extrait de {modified} : \", expr_with_dep_name)\n",
    "        dep_name = expr_with_dep_name.split(':')[-1].strip()\n",
    "        clean_dep_name = encode_name(dep_name)\n",
    "        target_template = encoded_dep_name2template[clean_dep_name]\n",
    "        if mapping[target_template] is None:\n",
    "            mapping[target_template] = modified\n",
    "        else:\n",
    "            duplicated_dep.append(dep_name)\n",
    "            print(f\"!!! {target_template} is not None -> probably duplicated \\n----See {modified}\")\n",
    "            \n",
    "    print(\"Fiches dupliquées (à retirer manuellement puis relancer le script) :\\n\", duplicated_dep)\n",
    "    print(f\"{len(mapping)} hits\")\n",
    "    return mapping\n",
    "\n",
    "\n",
    "template2modified_docx = map_templates_to_modified_reports(templates, modified_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les nouveaux documents contiennent le texte transposé et sous le même nom que leur template mais\n",
    "# dans un dossier différent\n",
    "\n",
    "\n",
    "def transpose_modification_to_new_reports(template2modified_docx):\n",
    "    # Transpose le texte ajouté aux documents sur le template associé. \n",
    "    # La correspondance se fait à partir du mapping (dictionnaire template -> doc modifié)\n",
    "    hit, unhit = 0, 0\n",
    "    dict_cont = {}\n",
    "    for template_path, modified_docx_path in template2modified_docx.items():\n",
    "        output_basename = template_path.split(os.sep)[-1]\n",
    "        output_path = os.path.join(transposed_docx_dir, output_basename)\n",
    "\n",
    "        output_name, dict_cont = fill_template(template_path, os.path.join(os.getcwd(), 'Fiche_Avant_Osmose', output_basename), volet2mesures, dict_cont)\n",
    "        \n",
    "        if modified_docx_path is None:\n",
    "            unhit += 1\n",
    "            print(f'Pas de transposition pour {template_path}')\n",
    "            output_name, dict_cont = fill_template(template_path, output_path, volet2mesures, dict_cont)\n",
    "        \n",
    "        else:\n",
    "            print(f'Transpose {template_path} vers {output_path}')\n",
    "            try:\n",
    "                # On veut transposer les commentaire de modified_docx_path -> template_path\n",
    "                # src_filename est modified_docx_path\n",
    "                output_name, dict_cont = transpose_comments(modified_docx_path, template_path, output_path, volet2mesures, dict_cont)\n",
    "                hit += 1\n",
    "            except:\n",
    "                print(f\"** Transposition impossible. Génération d'une fiche vide dans {template_path}**\")\n",
    "                output_name, dict_cont = fill_template(template_path, output_path, volet2mesures, dict_cont)\n",
    "                unhit += 1\n",
    "                \n",
    "    print(f\"Hit : {hit} | Unhit : {unhit}\")\n",
    "    return dict_cont\n",
    "\n",
    "\n",
    "dict_cont = transpose_modification_to_new_reports(template2modified_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérie si on a bien 109 rapport (1 par département) + 1 pour le gitkeep\n",
    "assert len(os.listdir(transposed_docx_dir)) == 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "propilot",
   "language": "python",
   "name": "propilot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
