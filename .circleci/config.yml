---
version: 2.1
jobs:
    install-requirements:
      docker:
        - image: python:3.8
      steps:
        - checkout
        - restore_cache:
            keys:
              - dependencies-{{ checksum "requirements.txt"}}
        - run:
              name: Installation des requirements Python
              command: |
                  python3 -m venv venv
                  . venv/bin/activate
                  pip3 install -r requirements.txt
                  python -m ipykernel install
        - save_cache:
            key: dependencies-{{ checksum "requirements.txt"}}
            paths:
              - "venv"

    get-daily-data:
      docker:
        - image: python:3.8
      steps:
        - checkout
        - run: date +%F > date
        - run:
              name : Récupération des données ProPilot
              command: |
                  echo "$DFAKTO_PK_BASE64" | base64 -d > dfakto.key
                  chmod 400 dfakto.key
                  chmod 777 ssh_askpass.sh
                  DISPLAY=1 SSH_ASKPASS="./ssh_askpass.sh" ssh-add dfakto.key < /dev/null
                  #ssh-keyscan -H ${DFAKTO_URL} >> ~/.ssh/known_hosts
                  mkdir data || true
                  cd data
                  sftp -o StrictHostKeychecking=no -P ${DFAKTO_PORT} -i ../dfakto.key ${USER_NAME}@${DFAKTO_URL} \<<EOF
                  cd files
                  get *.csv
                  exit
        - save_cache:
           key: data-input-{{ checksum "date" }}
           paths:
             - "data"

    build-pp_dep-and-fiches:
        docker:
          - image: python:3.8
        steps:
          - checkout
          - run: date +%F > date
          - run:
                name: Installation des prérequis
                command: |
                    apt-get update
                    apt-get upgrade -y --no-install-recommends
                    apt-get install -y zip lftp
          - restore_cache:
              keys:
                - data-input-{{ checksum "date" }}
          - restore_cache:
              keys:
                - dependencies-{{ checksum "requirements.txt"}}
          - run:
                name: Chargement des données ProPilot
                no_output_timeout: 1h
                command: |
                    python3 -m venv venv
                    . venv/bin/activate
                    jupyter nbconvert --to python chargement_propilot.ipynb
                    python chargement_propilot.py
                    FILE=propilot.csv
                    if test -f "$FILE"; then
                        echo "$FILE successfuly created."
                    fi
          - persist_to_workspace:
              root: ./
              paths:
                - propilot.csv
          - run:
                name: Export de propilot.csv
                no_output_timeout: 1h
                command: |
                    echo -e $SSL_CERTIFICATE_LETS_ENCRYPT_HACK > ca-certs.crt
                    FILES=exports/*.csv
                    for f in $FILES
                      do

                      echo "Sending $f"

                      if test -f "$f"; then
                              echo "$f successfuly created."
                      else
                              echo "$f not found"
                              ls .
                      fi

                      lftp -u ${DEPLOY_USER}:${DEPLOY_PASSWORD} ${DEPLOY_HOST} -e "set ssl:ca-file ca-certs.crt ; set ftp:ssl-force true ; cd propilot ; put $f ; quit"
                      if [ $? -eq 0 ]; then
                              echo "$f successfuly transfered."
                      fi
                      done
          - run:
                name: Production des fiches parlementaires
                no_output_timeout: 1h
                command: |
                    python3 -m venv venv
                    . venv/bin/activate
                    python3 code/build_reports.py
          #          #zip -9 archive.zip *.pdf
          #          #git add archive.zip
          #          cd reports_word
          #          git add Suivi*.docx
          #          git config --global  user.email "circleci@circleci"
          #          git config --global user.name "CircleCI"
          #          git commit -m "add new docx reports [ci skip]"
          #          git push --set-upstream origin $CIRCLE_BRANCH
          # - run:
          #       name: Export des données en format csv
          #       no_output_timeout: 1h
          #       command: |
          #           jupyter nbconvert --to python export_csv.ipynb
          #           python export_csv.py
          #           FILES=exports/*.csv
          #           for f in $FILES
          #             do

          #             echo "Sending $f"

          #             if test -f "$f"; then
          #                     echo "$f successfuly created."
          #             else
          #                     echo "$f not found"
          #                     ls .
          #             fi

          #             lftp -u ${DEPLOY_USER}:${DEPLOY_PASSWORD} ${DEPLOY_HOST} -e "set ftp:ssl-force true ; cd propilot ; put $f ; quit"
          #             if [ $? -eq 0 ]; then
          #                     echo "$f successfuly transfered."
          #             fi
          #             done

    convert-pp_dep-to-json-france-relance:
        docker:
          - image: python:3.8
        steps:
          - checkout
          - run: date +%F > date
          - restore_cache:
              keys:
                - dependencies-{{ checksum "requirements.txt"}}
          - attach_workspace:
              at: ./
          - run:
                name: Génération des documents JSON à publier sur data-economie pour le TDB France Relance
                command: |
                    python3 -m venv venv
                    . venv/bin/activate
                    jupyter nbconvert --to python csvToJson.ipynb
                    python csvToJson.py
          - persist_to_workspace:
              root: ./
              paths:
                - france-relance-data-tableau-de-bord.txt

    publish-json-france-relance:
        docker:
          - image: python:3.8
        steps:
          - checkout
          - run: date +%F > date
          - attach_workspace:
              at: ./
          - run:
                name: Publication des documents JSON sur data-economie pour le TDB France Relance
                command: |
                  apt-get update
                  apt-get install -y zip lftp
                  echo -e $SSL_CERTIFICATE_LETS_ENCRYPT_HACK > ca-certs.crt
                  lftp -u ${DEPLOY_USER}:${DEPLOY_PASSWORD} ${DEPLOY_HOST} -e "set ssl:ca-file ca-certs.crt ; set ftp:ssl-force true ; put france-relance-data-tableau-de-bord.txt ; rm relance.txt ; mv france-relance-data-tableau-de-bord.txt relance.txt ; quit"
          - store_artifacts:
              path: ./propilot.csv
              destination: propilot.csv
          - store_artifacts:
              path: ./france-relance-data-tableau-de-bord.txt
              destination: france-relance-data-tableau-de-bord.txt


workflows:
  version: 2.1
  main:
    jobs:
      - install-requirements
      - build-pp_dep-and-fiches:
            requires:
              - install-requirements
      - convert-pp_dep-to-json-france-relance:
            requires:
              - build-pp_dep-and-fiches
      - publish-json-france-relance:
            requires:
              - convert-pp_dep-to-json-france-relance
            filters:
              branches:
                only:
                  - master

  daily:
    jobs:
      - get-daily-data
      - install-requirements
      - build-pp_dep-and-fiches:
            requires:
              - install-requirements
      - convert-pp_dep-to-json-france-relance:
            requires:
              - build-pp_dep-and-fiches
      - publish-json-france-relance:
            requires:
              - convert-pp_dep-to-json-france-relance
    triggers:
      - schedule:
          cron: 0 6 * * 6
          filters:
            branches:
              only:
                - master

  daily-data:
    jobs:
      - get-daily-data
    triggers:
      - schedule:
          cron: 0 5 * * 6
          filters:
            branches:
              only:
                - master
